{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all files in  the wiki folder\n",
    "\n",
    "We can create a list with the names of all files in the wiki folder using the `os.listdir()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_names = os.listdir(\"../../Data/wiki\")\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "math: Used for mathematical operations.\n",
    "functools: Provides higher-order functions that act on or return other functions.\n",
    "Pool from multiprocessing: Manages a pool of worker processes.\n",
    "'''\n",
    "import math\n",
    "import functools\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the first file\n",
    "\n",
    "Let's read the first file and print its contents. We need to join the name of the file with the `wiki` folder. We can do this using the `os.path.join()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../../Data/wiki\", file_names[0])) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the MapReduce function to this project\n",
    "\n",
    "We start by adding the MapReduce function so that we can use throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose: Divides the data into smaller chunks for parallel processing.make_chunks(data, num_chunks) Splits the input data into approximately equal-sized chunks.\n",
    "\n",
    "Parameters:\n",
    "    data: The data to be split.\n",
    "    num_chunks: The number of chunks to split the data into.\n",
    "    Returns: A list of data chunks.\n",
    "Implementation:\n",
    "    chunk_size: Calculates the size of each chunk using ceiling division.\n",
    "    Uses list comprehension to create chunks of data.\n",
    "'''\n",
    "def make_chunks(data, num_chunks):\n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose: Applies the map-reduce pattern to the data using multiple processes. map_reduce(data, num_processes, mapper, reducer): Implements the map-reduce pattern using the provided mapper and reducer functions.\n",
    "\n",
    "Parameters:\n",
    "    data: The data to be processed.\n",
    "    num_processes: The number of processes to use for parallel processing.\n",
    "    mapper: A function to apply to each chunk of data.\n",
    "    reducer: A function to combine the results from the mapper functions.\n",
    "Returns: The final reduced result.\n",
    "Implementation:\n",
    "    Splits the data into chunks using make_chunks.\n",
    "    Creates a pool of worker processes.\n",
    "    Applies the mapper function to each chunk in parallel using pool.map.\n",
    "    Closes the pool and waits for all processes to finish.\n",
    "    Combines the results from all chunks using the reducer function and functools.reduce.\n",
    "'''\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    pool = Pool(num_processes)\n",
    "    chunk_results = pool.map(mapper, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return functools.reduce(reducer, chunk_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the total number of lines on all files\n",
    "\n",
    "It was not required but can use MapReduce to count the total number of lines on all files in the wiki folder! If you did in some other way, that is fine as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_line_count(file_names):\n",
    "    total = 0\n",
    "    for fn in file_names:\n",
    "        with open(os.path.join(\"wiki\", fn)) as f:\n",
    "            total += len(f.readlines())\n",
    "    return total\n",
    "    \n",
    "def reduce_line_count(count1, count2):\n",
    "    return count1 + count2\n",
    "\n",
    "target = \"data\"\n",
    "map_reduce(file_names, 8, map_line_count, reduce_line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grep string function\n",
    "\n",
    "We defined a `mapreduce_grep_string()` function that takes two arguments as input:\n",
    "\n",
    "1. A path to a folder. In the case of this guided project we will only use it on the `wiki` folder but having this argument makes the function easier to reuse.\n",
    "\n",
    "2. The string that we want to find.\n",
    "\n",
    "The mapper function receives a chunk of filenames and calculates all occurrences of the target string on them. If a file contains no occurrences, we chose to not include an entry for that file in the result dictionary.\n",
    "\n",
    "The reducer function uses the `dict.update()` method to merge the result dictionaries.\n",
    "\n",
    "Note that the `target` variable will be defined outside and will be the string we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable is defined outside and contains the string \n",
    "def map_grep(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            if target in line:\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                results[fn].append(line_index)\n",
    "    return results\n",
    "\n",
    "def reduce_grep(lines1, lines2):\n",
    "    lines1.update(lines2)\n",
    "    return lines1\n",
    "\n",
    "def mapreduce_grep(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep, reduce_grep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the occurences of \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"data\"\n",
    "data_occurrences = mapreduce_grep(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allow for case insensitive matches\n",
    "\n",
    "We can allow case insensitive matches by converting both the target and the file contents to lowercase before we match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_insensitive(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            if target.lower() in line:\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                results[fn].append(line_index)\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_insensitive(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_insensitive, reduce_grep)\n",
    "\n",
    "target = \"data\"\n",
    "new_data_occurrences = mapreduce_grep_insensitive(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking that we find more matches\n",
    "\n",
    "We already stored the results into variables `data_occurrences` and `new_data_occurrences`.  To check that we find more matches with the second version of the algorithm, we can loop over the file names and print the length difference between the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 new matches on file wiki/Dragnet_(franchise).html\n",
      "Found 1 new matches on file wiki/Jazz_in_Turkey.html\n",
      "Found 2 new matches on file wiki/Kate_Harwood.html\n",
      "Found 1 new matches on file wiki/Rally_for_Democracy_and_Progress_(Benin).html\n",
      "Found 1 new matches on file wiki/Morning_Glory_(2010_film).html\n",
      "Found 2 new matches on file wiki/Jules_Verne_ATV.html\n",
      "Found 1 new matches on file wiki/Claudia_Neidig.html\n",
      "Found 2 new matches on file wiki/Gordon_Bau.html\n",
      "Found 1 new matches on file wiki/Colchester_Village_Historic_District.html\n",
      "Found 1 new matches on file wiki/Sahanpur.html\n",
      "Found 1 new matches on file wiki/Harry_Hill_Bandholtz.html\n",
      "Found 1 new matches on file wiki/Morgana_King.html\n",
      "Found 1 new matches on file wiki/Nuno_Leal_Maia.html\n",
      "Found 1 new matches on file wiki/Alex_Kurtzman.html\n",
      "Found 1 new matches on file wiki/Camp_Nelson_Confederate_Cemetery.html\n",
      "Found 1 new matches on file wiki/Dewoitine_D.21.html\n",
      "Found 1 new matches on file wiki/WLSR.html\n",
      "Found 7 new matches on file wiki/List_of_people_from_Bangor,_Maine.html\n",
      "Found 1 new matches on file wiki/Qalat_Kat.html\n",
      "Found 1 new matches on file wiki/83_(number).html\n",
      "Found 1 new matches on file wiki/Typhoon_Hester_(1952).html\n",
      "Found 1 new matches on file wiki/Salem-Auburn_Streets_Historic_District.html\n",
      "Found 1 new matches on file wiki/Copamyntis_infusella.html\n",
      "Found 1 new matches on file wiki/Syngenor.html\n",
      "Found 2 new matches on file wiki/Kim_Yong-hwa.html\n",
      "Found 1 new matches on file wiki/Failing_Office_Building.html\n",
      "Found 1 new matches on file wiki/Westchester,_Los_Angeles.html\n",
      "Found 1 new matches on file wiki/Blue_SWAT.html\n",
      "Found 1 new matches on file wiki/Bahmanabad-e_Olya.html\n",
      "Found 3 new matches on file wiki/Maniitsoq_structure.html\n",
      "Found 1 new matches on file wiki/Functoid.html\n",
      "Found 1 new matches on file wiki/Teiji_Ito.html\n",
      "Found 1 new matches on file wiki/Pictogram.html\n",
      "Found 1 new matches on file wiki/Kattukukke.html\n",
      "Found 1 new matches on file wiki/Devil_on_Horseback.html\n",
      "Found 1 new matches on file wiki/Saravan,_Gilan.html\n",
      "Found 1 new matches on file wiki/KMTZ.html\n",
      "Found 4 new matches on file wiki/List_of_molecular_graphics_systems.html\n",
      "Found 1 new matches on file wiki/Dean_Kukan.html\n",
      "Found 1 new matches on file wiki/L._Fry.html\n",
      "Found 3 new matches on file wiki/Code_page_1023.html\n",
      "Found 1 new matches on file wiki/Hayateumi_Hidehito.html\n",
      "Found 1 new matches on file wiki/Jack_Goes_Home.html\n",
      "Found 1 new matches on file wiki/Meleh_Kabud-e_Sofla.html\n",
      "Found 2 new matches on file wiki/Taipa_Houses%E2%80%93Museum.html\n",
      "Found 1 new matches on file wiki/Cryptographic_primitive.html\n",
      "Found 2 new matches on file wiki/Precorrin-6A_reductase.html\n",
      "Found 1 new matches on file wiki/Curtiss-Wright_Hangar_(Columbia,_South_Carolina).html\n",
      "Found 1 new matches on file wiki/Battle_of_Wattignies.html\n",
      "Found 1 new matches on file wiki/Acceptance_(Heroes).html\n",
      "Found 1 new matches on file wiki/Benny_Lee.html\n",
      "Found 1 new matches on file wiki/Medicago_murex.html\n",
      "Found 1 new matches on file wiki/Companys,_proc%C3%A9s_a_Catalunya.html\n",
      "Found 1 new matches on file wiki/Shabbir_Kumar.html\n",
      "Found 1 new matches on file wiki/Oldfield_Baby_Great_Lakes.html\n",
      "Found 1 new matches on file wiki/Old_Mill_Creek,_Illinois.html\n",
      "Found 1 new matches on file wiki/Avengers_Academy.html\n",
      "Found 2 new matches on file wiki/Agaritine_gamma-glutamyltransferase.html\n",
      "Found 1 new matches on file wiki/Amborella.html\n",
      "Found 1 new matches on file wiki/Swathi_Chinukulu.html\n",
      "Found 1 new matches on file wiki/Derek_Acorah.html\n",
      "Found 1 new matches on file wiki/Ek_Dil_Sau_Afsane.html\n",
      "Found 1 new matches on file wiki/Holly_Golightly_(comics).html\n",
      "Found 1 new matches on file wiki/Golabkhvaran.html\n",
      "Found 1 new matches on file wiki/Antibiotic_use_in_livestock.html\n",
      "Found 1 new matches on file wiki/Lower_Blackburn_Grade_Bridge.html\n",
      "Found 1 new matches on file wiki/Roxbury_Presbyterian_Church.html\n",
      "Found 1 new matches on file wiki/Imperial_Venus_(film).html\n",
      "Found 1 new matches on file wiki/Demographics_of_American_Samoa.html\n",
      "Found 1 new matches on file wiki/The_Future_(film).html\n",
      "Found 1 new matches on file wiki/Weiser_River.html\n",
      "Found 1 new matches on file wiki/Bias.html\n",
      "Found 1 new matches on file wiki/The_Gentleman_Without_a_Residence_(1915_film).html\n",
      "Found 1 new matches on file wiki/Vojin_%C4%86etkovi%C4%87.html\n",
      "Found 1 new matches on file wiki/Danish_Maritime_Safety_Administration.html\n",
      "Found 1 new matches on file wiki/Gulliver_Mickey.html\n",
      "Found 1 new matches on file wiki/Cobble_Hill,_Brooklyn.html\n",
      "Found 1 new matches on file wiki/Filip_Pyrochta.html\n",
      "Found 1 new matches on file wiki/List_of_Uzbek_films_of_2014.html\n",
      "Found 1 new matches on file wiki/Furto_di_sera_bel_colpo_si_spera.html\n",
      "Found 1 new matches on file wiki/Craig_Chester.html\n",
      "Found 1 new matches on file wiki/%C3%89cole_des_Mines_de_Douai.html\n",
      "Found 1 new matches on file wiki/Taylor_Williamson.html\n",
      "Found 1 new matches on file wiki/Jonathan_A._Goldstein.html\n",
      "Found 1 new matches on file wiki/Ordinary,_Virginia.html\n",
      "Found 1 new matches on file wiki/Smilax_laurifolia.html\n",
      "Found 1 new matches on file wiki/Don_Parsons_(ice_hockey).html\n",
      "Found 1 new matches on file wiki/Daniel_Cerone.html\n",
      "Found 1 new matches on file wiki/Sol_Eclipse.html\n",
      "Found 1 new matches on file wiki/1953%E2%80%9354_FA_Cup_qualifying_rounds.html\n",
      "Found 2 new matches on file wiki/Viva_Villa!.html\n",
      "Found 1 new matches on file wiki/King_Parker_House.html\n",
      "Found 1 new matches on file wiki/Pushkar.html\n",
      "Found 1 new matches on file wiki/Manhattan_Murder_Mystery.html\n",
      "Found 1 new matches on file wiki/Appa_(film).html\n",
      "Found 1 new matches on file wiki/Panchamrutham.html\n",
      "Found 1 new matches on file wiki/Table_Point_Formation.html\n",
      "Found 2 new matches on file wiki/Claire_Danes.html\n",
      "Found 1 new matches on file wiki/HD_90156.html\n",
      "Found 1 new matches on file wiki/West_Park_Bridge.html\n",
      "Found 1 new matches on file wiki/Urs_Burkart.html\n",
      "Found 1 new matches on file wiki/Embraer_Unidade_Gavi%C3%A3o_Peixoto_Airport.html\n",
      "Found 1 new matches on file wiki/Shoreyjeh-ye_Do.html\n",
      "Found 1 new matches on file wiki/Kokan_Colony.html\n",
      "Found 1 new matches on file wiki/PTPRS.html\n",
      "Found 1 new matches on file wiki/Don_Raye.html\n",
      "Found 1 new matches on file wiki/Peter_Collingwood.html\n",
      "Found 1 new matches on file wiki/Rudy:_The_Rudy_Giuliani_Story.html\n",
      "Found 2 new matches on file wiki/Shpolskii_matrix.html\n",
      "Found 1 new matches on file wiki/Doumanaba.html\n",
      "Found 1 new matches on file wiki/Lis_L%C3%B8wert.html\n",
      "Found 1 new matches on file wiki/Kul_Gul.html\n",
      "Found 1 new matches on file wiki/Bibiana_Beglau.html\n",
      "Found 1 new matches on file wiki/Wilhelm_Wagenfeld_House.html\n",
      "Found 1 new matches on file wiki/Tim_Spencer_(singer).html\n",
      "Found 1 new matches on file wiki/Mudramothiram.html\n",
      "Found 2 new matches on file wiki/The_Audacity_to_Podcast.html\n",
      "Found 1 new matches on file wiki/Lake_County_Examiner.html\n",
      "Found 1 new matches on file wiki/Ingrid_Guimar%C3%A3es.html\n",
      "Found 1 new matches on file wiki/Frost_Township,_Michigan.html\n",
      "Found 1 new matches on file wiki/Jon_Mullich.html\n",
      "Found 2 new matches on file wiki/List_of_Spaghetti_Western_films.html\n",
      "Found 1 new matches on file wiki/Julien_Boisselier.html\n",
      "Found 1 new matches on file wiki/Exploratorium_(film).html\n",
      "Found 1 new matches on file wiki/Mirisah.html\n",
      "Found 1 new matches on file wiki/Foulonia.html\n",
      "Found 1 new matches on file wiki/Tropical_sprue.html\n",
      "Found 1 new matches on file wiki/A_Beautiful_Valley.html\n",
      "Found 1 new matches on file wiki/C11orf30.html\n",
      "Found 1 new matches on file wiki/Wilson_Global_Explorer.html\n",
      "Found 1 new matches on file wiki/Winters-Wimberley_House.html\n",
      "Found 1 new matches on file wiki/Boardman_Township,_Mahoning_County,_Ohio.html\n",
      "Found 2 new matches on file wiki/Tomohiko_It%C5%8D_(director).html\n",
      "Found 1 new matches on file wiki/Brownfield_(software_development).html\n"
     ]
    }
   ],
   "source": [
    "for fn in new_data_occurrences:\n",
    "    if fn not in data_occurrences:\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]), fn))\n",
    "    elif len(new_data_occurrences[fn]) > len(data_occurrences[fn]):\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]) - len(data_occurrences[fn]), fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding match indexes on lines\n",
    "\n",
    "We need to solve a subproblem before we implement this one: Given a string and a target, find all occurrences of the target within that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 27, 65]\n"
     ]
    }
   ],
   "source": [
    "def find_match_indexes(line, target):\n",
    "    results = []\n",
    "    i = line.find(target, 0)\n",
    "    while i != -1:\n",
    "        results.append(i)\n",
    "        i = line.find(target, i + 1)\n",
    "    return results\n",
    "\n",
    "# Test implementation\n",
    "s = \"Data science is related to data mining, machine learning and big data.\".lower()\n",
    "print(find_match_indexes(s, \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding all match locations\n",
    "\n",
    "We can use any of the above functions to find all match locations. We will use the third one.\n",
    "\n",
    "After finding all indexes in one line, we need to create pairs by adding the line index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_match_indexes(file_names):\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        for line_index, line in enumerate(lines):\n",
    "            match_indexes = find_match_indexes(line, target.lower())\n",
    "            if fn not in results:\n",
    "                results[fn] = []\n",
    "            results[fn] += [(line_index, match_index) for match_index in match_indexes]\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_match_indexes(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_match_indexes, reduce_grep)\n",
    "\n",
    "target = \"science\"\n",
    "occurrences = mapreduce_grep_match_indexes(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the results\n",
    "\n",
    "Let's display the results. We will create a CSV file listing all occurrences. We will also show the text around each occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# How many character to show before and after the match\n",
    "context_delta = 30\n",
    "\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    rows = [[\"File\", \"Line\", \"Index\", \"Context\"]]\n",
    "    for fn in occurrences:\n",
    "        with open(fn) as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        for line, index in occurrences[fn]:\n",
    "            start = max(index - context_delta, 0)\n",
    "            end   = index + len(target) + context_delta\n",
    "            rows.append([fn, line, index, lines[line][start:end]])\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "      <th>Index</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wiki/Rally_for_Democracy_and_Progress_(Benin)....</td>\n",
       "      <td>155</td>\n",
       "      <td>40</td>\n",
       "      <td>f=\"/wiki/Outline_of_political_science#Politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>wiki/Rally_for_Democracy_and_Progress_(Benin)....</td>\n",
       "      <td>155</td>\n",
       "      <td>96</td>\n",
       "      <td>\" title=\"Outline of political science\"&gt;Other c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>208</td>\n",
       "      <td>507</td>\n",
       "      <td>century French &lt;a href=\"/wiki/Science-fiction\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>208</td>\n",
       "      <td>551</td>\n",
       "      <td>n\" class=\"mw-redirect\" title=\"Science-fiction\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>208</td>\n",
       "      <td>568</td>\n",
       "      <td>rect\" title=\"Science-fiction\"&gt;science-fiction&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>427</td>\n",
       "      <td>231</td>\n",
       "      <td>text\" href=\"http://www.futura-sciences.com/fr/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>nnés\"&lt;/a&gt; (in French). Futura Sciences&lt;span cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>427</td>\n",
       "      <td>831</td>\n",
       "      <td>ft_id=http%3A%2F%2Fwww.futura-sciences.com%2Ff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>427</td>\n",
       "      <td>971</td>\n",
       "      <td>15986-1%2F&amp;amp;rft.pub=Futura+Sciences&amp;amp;rft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>wiki/Jules_Verne_ATV.html</td>\n",
       "      <td>941</td>\n",
       "      <td>60</td>\n",
       "      <td>ogramme_for_Life_and_Physical_Sciences_in_Spac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                File  Line  Index  \\\n",
       "0  wiki/Rally_for_Democracy_and_Progress_(Benin)....   155     40   \n",
       "1  wiki/Rally_for_Democracy_and_Progress_(Benin)....   155     96   \n",
       "2                          wiki/Jules_Verne_ATV.html   208    507   \n",
       "3                          wiki/Jules_Verne_ATV.html   208    551   \n",
       "4                          wiki/Jules_Verne_ATV.html   208    568   \n",
       "5                          wiki/Jules_Verne_ATV.html   427    231   \n",
       "6                          wiki/Jules_Verne_ATV.html   427    427   \n",
       "7                          wiki/Jules_Verne_ATV.html   427    831   \n",
       "8                          wiki/Jules_Verne_ATV.html   427    971   \n",
       "9                          wiki/Jules_Verne_ATV.html   941     60   \n",
       "\n",
       "                                             Context  \n",
       "0  f=\"/wiki/Outline_of_political_science#Politics...  \n",
       "1  \" title=\"Outline of political science\">Other c...  \n",
       "2  century French <a href=\"/wiki/Science-fiction\"...  \n",
       "3  n\" class=\"mw-redirect\" title=\"Science-fiction\"...  \n",
       "4  rect\" title=\"Science-fiction\">science-fiction<...  \n",
       "5  text\" href=\"http://www.futura-sciences.com/fr/...  \n",
       "6  nnés\"</a> (in French). Futura Sciences<span cl...  \n",
       "7  ft_id=http%3A%2F%2Fwww.futura-sciences.com%2Ff...  \n",
       "8  15986-1%2F&amp;rft.pub=Futura+Sciences&amp;rft...  \n",
       "9  ogramme_for_Life_and_Physical_Sciences_in_Spac...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"results.csv\")\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
