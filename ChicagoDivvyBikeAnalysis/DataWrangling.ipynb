{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/odo/backends/pandas.py:102: FutureWarning: pandas.tslib is deprecated and will be removed in a future version.\n",
      "You can access NaTType as type(pandas.NaT)\n",
      "  @convert.register((pd.Timestamp, pd.Timedelta), (pd.tslib.NaTType, type(None)))\n"
     ]
    }
   ],
   "source": [
    "import geocoder\n",
    "import unicodecsv\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "from pygeocoder import Geocoder\n",
    "from uszipcode import ZipcodeSearchEngine\n",
    "import pandas as pd\n",
    "import blaze\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_columns_with_suffix(df):\n",
    "    # list comprehension of the cols that end with '_y'\n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)\n",
    "    to_drop = [x for x in df if x.endswith('_x')]\n",
    "    df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Lat and Long to  Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://pythonhosted.org/uszipcode/\n",
    "with open('Social_Economic_Info_By_Zipcode/latlongzip.csv', 'w') as csvfile:\n",
    "                fieldnames = ['lat', 'long', 'zipcode']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "with open('Social_Economic_Info_By_Zipcode/latlong.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for line in reader: \n",
    "          lat = float(line['lat'])\n",
    "          lon = float(line['long'])\n",
    "          res = search.by_coordinate(lat, lon, returns=1)\n",
    "          with open('Social_Economic_Info_By_Zipcode/latlongzip.csv', 'a') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow({'lat': lat, 'long': lon, 'zipcode': res[0][\"Zipcode\"] })\n",
    "        \n",
    "#search = ZipcodeSearchEngine()\n",
    "#zipcode = search.by_zipcode(\"60613\")\n",
    "#print(res[0][\"Zipcode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Census Group, Zipcode, GeoID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA SOURCE : http://looker-datablocks.s3-website-us-east-1.amazonaws.com/\n",
    "geoid_group_df = pd.read_csv('Social_Economic_Info_By_Zipcode/Geoid_group_mapping.csv',header=0)\n",
    "geoid_zipcode_df = pd.read_csv('Social_Economic_Info_By_Zipcode/Geoid_zipcode_mapping.csv',header=0)\n",
    "facts_df = pd.read_csv('Social_Economic_Info_By_Zipcode/fast_facts.csv',header=0)\n",
    "\n",
    "geoid_group_zip_df = geoid_group_df.merge(geoid_zipcode_df,left_on=['geoid11'], right_on=['GEOID'],how='inner')\n",
    "demographics_df = geoid_group_zip_df.merge(facts_df,left_on=['blkgrp_map_geoid'], right_on=['logrecno_bg_map_block_group'],how='inner')\n",
    "demographics_df.to_csv('Social_Economic_Info_By_Zipcode/demographics.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Main Divvy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse_dates = true to read dates automatically\n",
    "divvy_dataset_raw =pd.read_csv('Divvy/trip_id_weather/chicago-divvy-bicycle-sharing-data/data_raw.csv',parse_dates=True)\n",
    "\n",
    "#Ensure the dataframe has no duplicates\n",
    "divvy_dataset_raw = divvy_dataset_raw.drop_duplicates()\n",
    "\n",
    "#Lets work with 2015, 2016 datasets\n",
    "#divvy_dataset_raw = divvy_dataset_raw.loc[('year'== 2015) & ('year' == 2016),:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge zipcode data to the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "divvy_dataset = divvy_dataset_raw.copy()\n",
    "\n",
    "zipcode_df = pd.read_csv('Social_Economic_Info_By_Zipcode/latlongzip.csv',header=0)\n",
    "\n",
    "#Populate start station zipcode\n",
    "divvy_dataset=divvy_dataset.merge(zipcode_df,left_on=['latitude_start','longitude_start'], right_on=['lat','long'],how='left')\n",
    "divvy_dataset.rename(inplace=True, columns={\"zipcode\": \"zipcode_start\"})\n",
    "\n",
    "#Populate end station zipcode\n",
    "divvy_dataset=divvy_dataset.merge(zipcode_df,left_on=['latitude_end','longitude_end'], right_on=['lat','long'],how='left')\n",
    "divvy_dataset.rename(inplace=True, columns={\"zipcode\": \"zipcode_end\"})\n",
    "\n",
    "#Convert zipcodes to integer\n",
    "divvy_dataset['zipcode_start'].fillna(0).astype(int)\n",
    "divvy_dataset['zipcode_end'].fillna(0).astype(int)\n",
    "\n",
    "divvy_dataset = divvy_dataset.drop_duplicates()\n",
    "drop_columns_with_suffix(divvy_dataset)\n",
    "\n",
    "divvy_dataset.to_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between start and end station in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_in_meters 0.0        126949\n",
      "1455.0      25180\n",
      "867.0       20318\n",
      "1830.0      18780\n",
      "1659.0      16797\n",
      "1359.0      15381\n",
      "3093.0      14625\n",
      "904.0       14148\n",
      "1159.0      14117\n",
      "3023.0      13750\n",
      "4489.0      13103\n",
      "2488.0      13091\n",
      "3603.0      12697\n",
      "1108.0      12079\n",
      "653.0       11647\n",
      "1873.0      11457\n",
      "4969.0      11450\n",
      "949.0       11369\n",
      "1172.0      11117\n",
      "1485.0      10999\n",
      "1786.0      10892\n",
      "1366.0      10520\n",
      "488.0       10457\n",
      "3304.0      10417\n",
      "1854.0      10170\n",
      "4714.0       9913\n",
      "760.0        9848\n",
      "1112.0       9813\n",
      "2939.0       9713\n",
      "1051.0       9493\n",
      "            ...  \n",
      "9505.0          1\n",
      "14589.0         1\n",
      "14582.0         1\n",
      "9495.0          1\n",
      "11972.0         1\n",
      "9493.0          1\n",
      "11986.0         1\n",
      "12010.0         1\n",
      "12015.0         1\n",
      "12016.0         1\n",
      "14624.0         1\n",
      "11945.0         1\n",
      "11895.0         1\n",
      "14636.0         1\n",
      "14707.0         1\n",
      "11897.0         1\n",
      "9564.0          1\n",
      "14701.0         1\n",
      "11903.0         1\n",
      "14694.0         1\n",
      "14689.0         1\n",
      "14687.0         1\n",
      "14686.0         1\n",
      "14685.0         1\n",
      "14671.0         1\n",
      "11911.0         1\n",
      "14647.0         1\n",
      "9516.0          1\n",
      "14639.0         1\n",
      "26256.0         1\n",
      "Name: distance_in_meters, Length: 12711, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "distance_df = pd.read_csv('Social_Economic_Info_By_Zipcode/Bike_route_distance_between_stations.csv',header=0)\n",
    "divvy_dataset = divvy_dataset.merge(distance_df,left_on=['from_station_name','to_station_name'], right_on=['start_station','end_station'],how='left')\n",
    "divvy_dataset.rename(inplace=True, columns={\"distance\": \"distance_in_meters\"})\n",
    "\n",
    "distance_df = pd.read_csv('Social_Economic_Info_By_Zipcode/station_distance.csv',header=0)\n",
    "divvy_dataset = divvy_dataset.merge(distance_df,left_on=['latitude_start','longitude_start','latitude_end','longitude_end'], right_on=['station_start_latitude','station_start_longitude','station_end_latitude','station_end_longitude'],how='left')\n",
    "divvy_dataset.rename(inplace=True, columns={\"distance_meters\": \"distance_meters_by_coord\"})\n",
    "\n",
    "#distance_df = pd.read_csv('Social_Economic_Info_By_Zipcode/station_distance.csv',header=0)\n",
    "#divvy_dataset = divvy_dataset.merge(distance_df,left_on=['from_station_id','to_station_id'], right_on=['station_start','station_end'],how='inner')\n",
    "#divvy_dataset.rename(inplace=True, columns={\"distance_meters\": \"distance_meters_by_station\"})\n",
    "\n",
    "divvy_dataset[\"distance_in_meters\"].fillna(0).astype(int)\n",
    "divvy_dataset[\"distance_meters_by_coord\"].fillna(0).astype(int)\n",
    "#divvy_dataset[\"distance_meters_by_station\"].fillna(0).astype(int)\n",
    "\n",
    "#print('distance_meters_by_coord',divvy_dataset.groupby(divvy_dataset.distance_meters_by_coord).size())\n",
    "#print('distance_meters_by_tation',divvy_dataset.groupby(divvy_dataset.distance_meters_by_station).size())\n",
    "\n",
    "divvy_dataset.loc[divvy_dataset.distance_in_meters == 0.0,['distance_in_meters']] = divvy_dataset.distance_meters_by_coord\n",
    "\n",
    "divvy_dataset.drop_duplicates()\n",
    "drop_columns_with_suffix(divvy_dataset)\n",
    "divvy_dataset.to_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode_stationdistance.csv', sep=',')\n",
    "\n",
    "print('distance_in_meters',divvy_dataset.distance_in_meters.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Social and Economic attributes for 2014, 2015, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path at terminal when executing this file\n",
      "/Users/chetana/Documents/Python/Notebook/SpringBoard/Capstone1/Social_Economic_Info_By_Zipcode/ACS/2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('Social_Economic_Info_By_Zipcode/ACS/2014')\n",
    "print(\"Path at terminal when executing this file\")\n",
    "print(os.getcwd() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2014 Data\n",
    "Mapping_2014_df = pd.read_csv('Geoid_group_mapping.csv',header=0,delimiter=',')\n",
    "df1 = pd.read_csv('1.csv',header=0,delimiter=',')\n",
    "df2 = pd.read_csv('2.csv',header=0,delimiter=',')\n",
    "df3 = pd.read_csv('3.csv',header=0,delimiter=',')\n",
    "df4 = pd.read_csv('4.csv',header=0,delimiter=',')\n",
    "df5 = pd.read_csv('5.csv',header=0,delimiter=',')\n",
    "df6 = pd.read_csv('6.csv',header=0,delimiter=',')\n",
    "df7 = pd.read_csv('7.csv',header=0,delimiter=',')\n",
    "\n",
    "dem_inc_pop_race_trp_2014=Mapping_2014_df.merge(df1,left_on=['blkgrp_map_geoid'], right_on=['Id2'],how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df2,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df3,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df4,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df5,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df6,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014 = dem_inc_pop_race_trp_2014.merge(df7,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2014['year'] = 2014\n",
    "\n",
    "dem_inc_pop_race_trp_2014.to_csv('../../../Social_Economic_Info_By_Zipcode/MergedData/dem_inc_pop_race_trp_2014.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2015 Data\n",
    "Mapping_2015_df = pd.read_csv('Geoid_group_mapping.csv',header=0,delimiter=',')\n",
    "df1 = pd.read_csv('1.csv',header=0,delimiter=',')\n",
    "df2 = pd.read_csv('2.csv',header=0,delimiter=',')\n",
    "df3 = pd.read_csv('3.csv',header=0,delimiter=',')\n",
    "df4 = pd.read_csv('4.csv',header=0,delimiter=',')\n",
    "df5 = pd.read_csv('5.csv',header=0,delimiter=',')\n",
    "df6 = pd.read_csv('6.csv',header=0,delimiter=',')\n",
    "df7 = pd.read_csv('7.csv',header=0,delimiter=',')\n",
    "\n",
    "dem_inc_pop_race_trp_2015=Mapping_2015_df.merge(df1,left_on=['blkgrp_map_geoid'], right_on=['Id2'],how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df2,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df3,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df4,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df5,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df6,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015 = dem_inc_pop_race_trp_2015.merge(df7,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2015['year'] = 2015\n",
    "\n",
    "dem_inc_pop_race_trp_2015.to_csv('../../../Social_Economic_Info_By_Zipcode/MergedData/dem_inc_pop_race_trp_2015.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2016 Data\n",
    "Mapping_2016_df = pd.read_csv('Geoid_group_mapping.csv',header=0,delimiter=',')\n",
    "df1 = pd.read_csv('1.csv',header=0,delimiter=',')\n",
    "df2 = pd.read_csv('2.csv',header=0,delimiter=',')\n",
    "df3 = pd.read_csv('3.csv',header=0,delimiter=',')\n",
    "df4 = pd.read_csv('4.csv',header=0,delimiter=',')\n",
    "df5 = pd.read_csv('5.csv',header=0,delimiter=',')\n",
    "df6 = pd.read_csv('6.csv',header=0,delimiter=',')\n",
    "df7 = pd.read_csv('7.csv',header=0,delimiter=',')\n",
    "\n",
    "dem_inc_pop_race_trp_2016=Mapping_2016_df.merge(df1,left_on=['blkgrp_map_geoid'], right_on=['Id2'],how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df2,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df3,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df4,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df5,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df6,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016 = dem_inc_pop_race_trp_2016.merge(df7,left_on='blkgrp_map_geoid',right_on='Id2',how='inner')\n",
    "dem_inc_pop_race_trp_2016['year'] = 2016\n",
    "\n",
    "dem_inc_pop_race_trp_2016.to_csv('../../../Social_Economic_Info_By_Zipcode/MergedData/dem_inc_pop_race_trp_2016.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Date Attributes to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "divvy_dataset = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode_stationdistance.csv', sep=',')\n",
    "\n",
    "divvy_dataset.stoptime=pd.to_datetime(divvy_dataset.stoptime)\n",
    "divvy_dataset.starttime=pd.to_datetime(divvy_dataset.starttime)\n",
    "divvy_dataset['Year'] = divvy_dataset['starttime'].apply(lambda d: d.year)\n",
    "divvy_dataset['Month'] = divvy_dataset['starttime'].apply(lambda d: d.month)\n",
    "divvy_dataset['Day'] = divvy_dataset['starttime'].apply(lambda d: d.day)\n",
    "divvy_dataset['DayOfWeek'] = divvy_dataset['starttime'].apply(lambda d: d.dayofweek)\n",
    "divvy_dataset['DayName'] = divvy_dataset['starttime'].apply(lambda d: d.weekday_name)\n",
    "divvy_dataset['DayOfYear'] = divvy_dataset['starttime'].apply(lambda d: d.dayofyear)\n",
    "divvy_dataset['WeekOfYear'] = divvy_dataset['starttime'].apply(lambda d: d.weekofyear)\n",
    "divvy_dataset['Quarter'] = divvy_dataset['starttime'].apply(lambda d: d.quarter)\n",
    "\n",
    "divvy_dataset.to_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode_stationdistance.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add zipcode to social and economic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chetana/Documents/Python/Notebook/SpringBoard/Capstone1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 556 entries, 0 to 555\n",
      "Data columns (total 60 columns):\n",
      "logrecno                                   470 non-null float64\n",
      "tract_x                                    470 non-null float64\n",
      "blkgrp                                     470 non-null float64\n",
      "geoid11                                    470 non-null float64\n",
      "blkgrp_map_geoid                           470 non-null float64\n",
      "tract_name                                 470 non-null object\n",
      "block_group_name                           470 non-null object\n",
      "latitude                                   470 non-null float64\n",
      "longitude                                  470 non-null float64\n",
      "square_miles_land                          470 non-null float64\n",
      "square_miles_water                         470 non-null float64\n",
      "year                                       470 non-null float64\n",
      "total_population                           470 non-null float64\n",
      "median_household_income                    470 non-null object\n",
      "per_capita_income                          470 non-null object\n",
      "median_age_total                           470 non-null object\n",
      "median_age_male                            470 non-null object\n",
      "median_age_female                          470 non-null object\n",
      "race_total                                 470 non-null float64\n",
      "income_less_than_$10000                    470 non-null float64\n",
      "income_$10000_to_$14999                    470 non-null float64\n",
      "income_$15000_to_$19999                    470 non-null float64\n",
      "income_$20000_to_$24999                    470 non-null float64\n",
      "income_$25000_to_$29999                    470 non-null float64\n",
      "income_$30000_to_$34999                    470 non-null float64\n",
      "income_$35000_to_$39999                    470 non-null float64\n",
      "income_$40000_to_$44999                    470 non-null float64\n",
      "income_$45000_to_$49999                    470 non-null float64\n",
      "income_$50000_to_$59999                    470 non-null float64\n",
      "income_$60000_to_$74999                    470 non-null float64\n",
      "income_$75000_to_$99999                    470 non-null float64\n",
      "income_$100000_to_$124999                  470 non-null float64\n",
      "income_$125000_to_$149999                  470 non-null float64\n",
      "income_$150000_to_$199999                  470 non-null float64\n",
      "income_$200000_or_more                     470 non-null float64\n",
      "aggregate_travel_time_in_minutes           470 non-null object\n",
      "one_person_carpool                         470 non-null object\n",
      "two_person_carpool                         470 non-null object\n",
      "three_person_carpool                       470 non-null object\n",
      "public_transportation                      470 non-null object\n",
      "Walked                                     470 non-null object\n",
      "taxi_motorcycle_bike                       470 non-null object\n",
      "White_alone                                470 non-null float64\n",
      "Black_or_African_American_alone            470 non-null float64\n",
      "American_Indian_and_Alaska_Native_alone    470 non-null float64\n",
      "Asian_alone                                470 non-null float64\n",
      "some_other_race_alone                      470 non-null float64\n",
      "two_or_more_races                          470 non-null float64\n",
      "zipcode                                    556 non-null int64\n",
      "tract_y                                    470 non-null float64\n",
      "geoid                                      470 non-null float64\n",
      "zpoppct                                    470 non-null float64\n",
      "Unnamed: 0                                 0 non-null float64\n",
      "school_score                               330 non-null float64\n",
      "school_name                                330 non-null object\n",
      "school_address                             330 non-null object\n",
      "school_city                                330 non-null object\n",
      "school_zip_code                            330 non-null float64\n",
      "school_enrollment                          330 non-null float64\n",
      "school_grades                              309 non-null object\n",
      "dtypes: float64(41), int64(1), object(18)\n",
      "memory usage: 265.0+ KB\n"
     ]
    }
   ],
   "source": [
    "soc_eco_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/SourceData/social_economic_attributes.csv', header=0,sep=',')\n",
    "zipcode_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/SourceData/Geoid_zipcode_mapping.csv', header=0,sep=',')\n",
    "school_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/SourceData/school_information.csv', header=0,sep=',')\n",
    "\n",
    "soc_eco_df.columns = soc_eco_df.columns.str.strip()\n",
    "zipcode_df.columns = zipcode_df.columns.str.strip()\n",
    "school_df.columns = school_df.columns.str.strip()\n",
    "school_df['school_zip_code'] = school_df['school_zip_code'].fillna(0).astype(int)\n",
    "\n",
    "soc_eco_df = soc_eco_df.merge(zipcode_df,left_on='geoid11',right_on='geoid',how='left')\n",
    "soc_eco_df.drop_duplicates()\n",
    "soc_eco_df = soc_eco_df.merge(school_df,left_on='zipcode',right_on='school_zip_code',how='outer')\n",
    "soc_eco_df.drop_duplicates()\n",
    "\n",
    "soc_eco_df['zipcode'] = soc_eco_df['zipcode'].fillna(0).astype(int)\n",
    "soc_eco_df.to_csv('Social_Economic_Info_By_Zipcode/MergedData/social_economic_zipcode_attributes.csv', sep=',')\n",
    "\n",
    "soc_eco_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-a75939eb4250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "#You need a dictionary that maps each key to multiple values.\n",
    "csvfile='Social_Economic_Info_By_Zipcode/MergedData/SourceData/zipcode_multiple_geoids.csv'\n",
    "dict_to_csvfile='Social_Economic_Info_By_Zipcode/MergedData/SourceData/dict_zipcode_multiple_geoids.csv'\n",
    "dict_transpose_to_csvfile='Social_Economic_Info_By_Zipcode/MergedData/SourceData/dict_transpose_zipcode_multiple_geoids.csv'\n",
    "\n",
    "d1 = {}\n",
    "with open(csvfile, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        d1.setdefault(row[0], []).append(row[1])\n",
    "\n",
    "with open(dict_to_csvfile, 'w') as f:  # Just use 'w' mode in 3.x\n",
    "    w = csv.DictWriter(f, d1.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(d1)\n",
    "\n",
    "## Transpose rows to columns (not tested)    \n",
    "#from more_itertools import izip\n",
    "#a = izip(*csv.reader(open(dict_to_csvfile, \"r\")))\n",
    "#csv.writer(open(dict_transpose_to_csvfile, \"wb\")).writerows(a)\n",
    "\n",
    "# Requires array of same length\n",
    "#pd.DataFrame.from_dict(d1)\n",
    "\n",
    "geoid_zip_map = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in d1.items() ]))\n",
    "\n",
    "x = list(geoid_zip_map['60601'].dropna())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Divvy and Social Economic Datasets into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "divvy_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode_stationdistance.csv', sep=',')\n",
    "soc_eco_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/social_economic_zipcode_attributes.csv', sep=',')\n",
    "\n",
    "#Drop Duplicates\n",
    "divvy_df.drop_duplicates()\n",
    "\n",
    "#Create a copy to maintain the original\n",
    "divvy_orig_df = divvy_df.copy()\n",
    "\n",
    "#Changes in the file\n",
    "divvy_df['zipcode_start'] = divvy_df['zipcode_start'].fillna(0).astype(int)\n",
    "divvy_df['zipcode_end'] = divvy_df['zipcode_end'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check for missing values for Year Column\n",
    "divvy_df.loc[divvy_df.Year.isnull(),:]\n",
    "divvy_df.Year.isnull().values.any()\n",
    "#####False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep 2014, 2015, 2016 year data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipcode_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/SourceData/transpose_dict_zipcode_multiple_geoids.csv', header=0,sep=';')\n",
    "zipcode_df['zipcode'] = zipcode_df['zipcode'].fillna(0).astype(int)\n",
    "divvy_df['zipcode_start'] = divvy_df['zipcode_start'].fillna(0).astype(int)\n",
    "divvy_df = divvy_df.merge(zipcode_df,left_on='zipcode_start',right_on='zipcode',how='left')\n",
    "\n",
    "divvy_df_2013 = divvy_df.loc[divvy_df.Year == 2013,:]\n",
    "divvy_df_2017 = divvy_df.loc[divvy_df.Year == 2017,:]\n",
    "divvy_13_17_df = pd.concat([divvy_df_2013, divvy_df_2017])\n",
    "\n",
    "#Divvy Data for 2014, 2015, 2016\n",
    "divvy_df_2014 = divvy_df.loc[divvy_df.Year == 2014,:]\n",
    "divvy_df_2015 = divvy_df.loc[divvy_df.Year == 2015,:]\n",
    "divvy_df_2016 = divvy_df.loc[divvy_df.Year == 2016,:]\n",
    "\n",
    "divvy_df.drop_duplicates()\n",
    "\n",
    "#Remove data from dataframe\n",
    "#pd.concat([divvy_df, divvy_13_17_df]).drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Divvy and Social Economic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Social Economic Data for 2014, 2015, 2016\n",
    "soc_eco_df = pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/social_economic_zipcode_attributes.csv', sep=',')\n",
    "soc_eco_df['zipcode'] = soc_eco_df['zipcode'].fillna(0).astype(int)\n",
    "\n",
    "soc_eco_df_2014 = soc_eco_df.loc[soc_eco_df.year == 2014,:]\n",
    "soc_eco_df_2015 = soc_eco_df.loc[soc_eco_df.year == 2015,:]\n",
    "soc_eco_df_2016 = soc_eco_df.loc[soc_eco_df.year == 2016,:]\n",
    "\n",
    "soc_eco_df_2014.zipcode.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#YEAR : No of Records : Size of File\n",
    "#2014 : 3783945 : 2.8GB\n",
    "#2015 : 4717366 : 3.32GB\n",
    "#2016 : 5309852 : 3.62GB\n",
    "\n",
    "#2014\n",
    "divvy_soc_eco_2014=divvy_df_2014.merge(soc_eco_df_2014,left_on=['zipcode_start'], right_on=['zipcode'],how='left')\n",
    "divvy_soc_eco_2014.drop(['geoid','logrecno','blkgrp','geoid11','blkgrp_map_geoid','latitude','longitude','block_group_name','tract_name','city','station_start','station_end','distance_meters_by_coord','station_start_latitude','station_start_longitude','station_end_latitude','station_end_longitude','start_station','end_station','station_start','station_end'],inplace=True,axis=1)\n",
    "divvy_soc_eco_2014.drop_duplicates()\n",
    "drop_columns_with_suffix(divvy_soc_eco_2014)\n",
    "divvy_soc_eco_2014.to_csv('Social_Economic_Info_By_Zipcode/MergedData/Divvy_2014_merged.csv')\n",
    "\n",
    "#2015\n",
    "divvy_soc_eco_2015=divvy_df_2015.merge(soc_eco_df_2015,left_on=['zipcode_start'], right_on=['zipcode'],how='left')\n",
    "divvy_soc_eco_2015.drop(['geoid','logrecno','blkgrp','geoid11','blkgrp_map_geoid','latitude','longitude','block_group_name','tract_name','city','station_start','station_end','distance_meters_by_coord','station_start_latitude','station_start_longitude','station_end_latitude','station_end_longitude','start_station','end_station','station_start','station_end'],inplace=True,axis=1)\n",
    "divvy_soc_eco_2015.drop_duplicates()\n",
    "drop_columns_with_suffix(divvy_soc_eco_2015)\n",
    "divvy_soc_eco_2015.to_csv('Social_Economic_Info_By_Zipcode/MergedData/Divvy_2015_merged.csv')\n",
    "\n",
    "#2016\n",
    "divvy_soc_eco_2016=divvy_df_2016.merge(soc_eco_df_2016,left_on=['zipcode_start'], right_on=['zipcode'],how='left')\n",
    "divvy_soc_eco_2016.drop(['geoid','logrecno','blkgrp','geoid11','blkgrp_map_geoid','latitude','longitude','block_group_name','tract_name','city','station_start','station_end','distance_meters_by_coord','station_start_latitude','station_start_longitude','station_end_latitude','station_end_longitude','start_station','end_station','station_start','station_end'],inplace=True,axis=1)\n",
    "divvy_soc_eco_2016.drop_duplicates()\n",
    "drop_columns_with_suffix(divvy_soc_eco_2016)\n",
    "divvy_soc_eco_2016.to_csv('Social_Economic_Info_By_Zipcode/MergedData/Divvy_2016_merged.csv')\n",
    "\n",
    "#divvy_soc_eco_2014.loc[divvy_soc_eco_2014.zipcode_start == 60601,:].to_csv('Social_Economic_Info_By_Zipcode/MergedData/Divvy_2014_merged_60601.csv')\n",
    "\n",
    "#divvy_14_15_16_df = pd.concat([divvy_soc_eco_2014, divvy_soc_eco_2015, divvy_soc_eco_2016])\n",
    "#divvy_14_15_16_df.to_csv('Social_Economic_Info_By_Zipcode/MergedData/Divvy_Social_Economic.csv')\n",
    "\n",
    "#Get rows with NANs\n",
    "#nans = lambda df: df[df.isnull().any(axis=1)]\n",
    "#nans(divvy_soc_eco_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'trip_id'\n",
    ",'usertype'\n",
    ",'gender'\n",
    ",'starttime'\n",
    ",'stoptime'\n",
    ",'tripduration'\n",
    ",'from_station_id'\n",
    ",'to_station_id'\n",
    ",'from_station_name'\n",
    ",'to_station_name'\n",
    ",'dpcapacity_start'\n",
    ",'latitude_start'\n",
    ",'longitude_start'\n",
    ",'latitude_end'\n",
    ",'longitude_end'\n",
    ",'dpcapacity_end'\n",
    ",'temperature'\n",
    ",'windchill'\n",
    ",'dewpoint'\n",
    ",'humidity'\n",
    ",'pressure'\n",
    ",'visibility'\n",
    ",'wind_speed'\n",
    ",'precipitation'\n",
    ",'events'\n",
    ",'rain'\n",
    ",'conditions'\n",
    ",'zipcode_start'\n",
    ",'zipcode_end'\n",
    ",'distance_in_meters'\n",
    ",'Year'\n",
    ",'Month'\n",
    ",'Day'\n",
    ",'DayOfWeek'\n",
    ",'DayName'\n",
    ",'DayOfYear'\n",
    ",'WeekOfYear'\n",
    ",'Quarter'\n",
    ",'geoid_list'\n",
    ",'square_miles_land'\n",
    ",'square_miles_water'\n",
    ",'year'\n",
    ",'total_population'\n",
    ",'median_household_income'\n",
    ",'per_capita_income'\n",
    ",'median_age_total'\n",
    ",'median_age_male'\n",
    ",'median_age_female'\n",
    ",'race_total'\n",
    ",'income_less_than_$10000'\n",
    ",'income_$10000_to_$14999'\n",
    ",'income_$15000_to_$19999'\n",
    ",'income_$20000_to_$24999'\n",
    ",'income_$25000_to_$29999'\n",
    ",'income_$30000_to_$34999'\n",
    ",'income_$35000_to_$39999'\n",
    ",'income_$40000_to_$44999'\n",
    ",'income_$45000_to_$49999'\n",
    ",'income_$50000_to_$59999'\n",
    ",'income_$60000_to_$74999'\n",
    ",'income_$75000_to_$99999'\n",
    ",'income_$100000_to_$124999'\n",
    ",'income_$125000_to_$149999'\n",
    ",'income_$150000_to_$199999'\n",
    ",'income_$200000_or_more'\n",
    ",'aggregate_travel_time_in_minutes'\n",
    ",'one_person_carpool'\n",
    ",'two_person_carpool'\n",
    ",'three_person_carpool'\n",
    ",'public_transportation'\n",
    ",'Walked'\n",
    ",'taxi_motorcycle_bike'\n",
    ",'White_alone',\n",
    ",'Black_or_African_American_alone'\n",
    ",'American_Indian_and_Alaska_Native_alone'\n",
    ",'Asian_alone'\n",
    ",'some_other_race_alone'\n",
    ",'two_or_more_races'\n",
    ",'zpoppct'\n",
    ",'school_score'\n",
    ",'school_name'\n",
    ",'school_address'\n",
    ",'school_city'\n",
    ",'school_zip_code'\n",
    ",'school_enrollment'\n",
    ",'school_grades'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add name of the month using integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse_dates = true to read dates automatically\n",
    "divvyTrips =pd.read_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_raw_zipcode_stationdistance.csv',parse_dates=True)\n",
    "\n",
    "#Ensure the dataframe has no duplicates\n",
    "divvyTrips = divvyTrips.drop_duplicates()\n",
    "#Remove all rows with missing values. axis =0 is the default \n",
    "divvyTrips = divvyTrips.dropna(subset=['trip_id'], how=\"all\", axis=0)\n",
    "\n",
    "months = {1: 'Jan', 2: 'Feb', 3: 'Mar',4: 'Apr',5:'May',6: 'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12: 'Dec'}\n",
    "\n",
    "divvyTrips['month_name'] = divvyTrips['Month'].map(months)\n",
    "divvyTrips['months-year'] = divvyTrips.reset_index().apply(lambda x: '{}-{}'.format(x['month_name'],x['Year']),axis=1)\n",
    "\n",
    "#divvy_dataset['month-year'] = divvy_dataset.reset_index().apply(lambda x: '{}-{}'.format(x['months'],x['year']),axis=1)\n",
    "\n",
    "#Convert Trip to minutes\n",
    "divvyTrips['tripInMinutes'] = divvyTrips['tripduration'] / 60\n",
    "\n",
    "#Add the Hour column\n",
    "divvyTrips.starttime = pd.to_datetime(divvyTrips.starttime)\n",
    "divvyTrips['Hour'] = divvyTrips.starttime.apply(lambda x: x.hour)\n",
    "\n",
    "## If trip start time is greater than stop time, swap them\n",
    "divvyTrips.loc[divvyTrips.starttime > divvyTrips.stoptime,['starttime','stoptime']] = divvyTrips.loc[divvyTrips.starttime > divvyTrips.stoptime,['stoptime','starttime']].values\n",
    "\n",
    "#Drop columns \n",
    "divvyTrips.drop(['Unnamed: 0','Unnamed: 0.1'],inplace=True,axis=1)\n",
    "\n",
    "#Remove Dependent user type\n",
    "divvyTrips = divvyTrips[divvyTrips.usertype != 'Dependent']\n",
    "\n",
    "# Drop Duplicates\n",
    "divvyTrips = divvyTrips.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Station capacity into buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert an existing Series or column to a category dtype:\n",
    "divvyTrips['gender'].astype('category')\n",
    "\n",
    "int(divvyTrips.dpcapacity_start.max())\n",
    "##### 55\n",
    "\n",
    "labels = [\"{0} - {1}\".format(i, i + 9) for i in range(int(divvyTrips.dpcapacity_start.min()), int(divvyTrips.dpcapacity_start.max())*2, 10)]\n",
    "\n",
    "#divvy_dataset['group'] = pd.cut(divvy_dataset.dpcapacity_start, range(int(divvy_dataset.dpcapacity_start.min()), int(divvy_dataset.dpcapacity_start.max()), 10), right=False, labels=labels)\n",
    "divvyTrips['start_station_group'] = pd.cut(divvyTrips.dpcapacity_start, range(0, 120, 10), right=False, labels=labels)\n",
    "divvyTrips.head()\n",
    "\n",
    "divvyTrips['end_station_group'] = pd.cut(divvyTrips.dpcapacity_end, range(0, 120, 10), right=False, labels=labels)\n",
    "divvyTrips.head()\n",
    "\n",
    "stations_trips = divvyTrips.groupby('start_station_group')['trip_id','gender'].agg(len).dropna()\n",
    "\n",
    "stations_trips.plot(kind='bar');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Holiday, weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=divvyTrips['starttime'].min(), end=divvyTrips['starttime'].max())\n",
    "divvyTrips['TypeOfDay'] = divvyTrips['starttime'].dt.date.astype('datetime64').isin(holidays)\n",
    "divvyTrips.loc[(divvyTrips.DayOfWeek == 5) | (divvyTrips.DayOfWeek == 6) | (divvyTrips.TypeOfDay == True),'TypeOfDay'] = 'Holiday'\n",
    "divvyTrips.loc[(divvyTrips.TypeOfDay == False),'TypeOfDay'] = 'Weekday'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Seasons to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = ['Winter', 'Winter', 'Winter', 'Spring', 'Spring', 'Summer', 'Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter']\n",
    "month_to_season = dict(zip(range(1,13), seasons))\n",
    "divvyTrips['season']=divvyTrips.starttime.dt.month.map(month_to_season) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "divvyTrips.to_csv('Social_Economic_Info_By_Zipcode/MergedData/divvy_trips_zipcode_stationdistance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "'trip_id'\n",
    ",'usertype'\n",
    ",'gender'\n",
    ",'starttime'\n",
    ",'stoptime'\n",
    ",'tripduration'\n",
    ",'from_station_id'\n",
    ",'to_station_id'\n",
    ",'from_station_name'\n",
    ",'to_station_name'\n",
    ",'dpcapacity_start'\n",
    ",'latitude_start'\n",
    ",'longitude_start'\n",
    ",'latitude_end'\n",
    ",'longitude_end'\n",
    ",'dpcapacity_end'\n",
    ",'temperature'\n",
    ",'windchill'\n",
    ",'dewpoint'\n",
    ",'humidity'\n",
    ",'pressure'\n",
    ",'visibility'\n",
    ",'wind_speed'\n",
    ",'precipitation'\n",
    ",'events'\n",
    ",'rain'\n",
    ",'conditions'\n",
    ",'zipcode_start'\n",
    ",'zipcode_end'\n",
    ",'distance_in_meters'\n",
    ",'Year'\n",
    ",'Month'\n",
    ",'Day'\n",
    ",'DayOfWeek'\n",
    ",'DayName'\n",
    ",'DayOfYear'\n",
    ",'WeekOfYear'\n",
    ",'Quarter'\n",
    ",'month_name'\n",
    ",'months-year'\n",
    ",'tripInMinutes'\n",
    ",'start_station_group'\n",
    ",'end_station_group'\n",
    ",'geometry_start'\n",
    ",'geometry_end'\n",
    "]\n",
    "divvyTrips[columns_to_keep].head()\n",
    "divvyTrips.filter(columns_to_keep).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trip_cost_15_16_17(trip):\n",
    "\tif trip['usertype']=='Subscriber':\n",
    "\t\tcost = 0\n",
    "\t\tif (trip['tripduration'] > 30*60):\n",
    "\t\t\t# 30-60 minutes: $3.00\n",
    "\t\t\tcost += 1.5\n",
    "\t\tif (trip['tripduration'] > 60*60):\n",
    "\t\t\t# 60-90 minutes: +$4.50\n",
    "\t\t\tcost += 3.0\n",
    "\t\tif (trip['tripduration'] > 90*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 90*60\n",
    "\t\t\tcost += 10.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 120*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 120*60\n",
    "\t\t\tcost += 16.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 150*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 150*60\n",
    "\t\t\tcost += 22.50*(addl_duration/(30.0*60))\n",
    "def trip_cost(trip):\n",
    "\tif trip['usertype']=='Subscriber':\n",
    "\t\tcost = 0\n",
    "\t\tif (trip['tripduration'] > 30*60) & (trip['tripduration'] <= 60*60):\n",
    "\t\t\t# 30-60 minutes: $3.00\n",
    "\t\t\tcost += 1.5\n",
    "\t\tif (trip['tripduration'] > 60*60) & (trip['tripduration'] <= 90*60):\n",
    "\t\t\t# 60-90 minutes: +$4.50\n",
    "\t\t\tcost += 3.0\n",
    "\t\tif (trip['tripduration'] > 90*60) & (trip['tripduration'] <= 120*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 90*60\n",
    "\t\t\tcost += 10.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 120*60) & (trip['tripduration'] <= 150*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 120*60\n",
    "\t\t\tcost += 16.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 150*60) & (trip['tripduration'] <= 180*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 150*60\n",
    "\t\t\tcost += 22.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 180*60) & (trip['tripduration'] <= 210*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 180*60\n",
    "\t\t\tcost += 28.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 210*60) & (trip['tripduration'] <= 240*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 210*60\n",
    "\t\t\tcost += 34.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 240*60) & (trip['tripduration'] <= 270*60):\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $10.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 240*60\n",
    "\t\t\tcost += 40.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 270*60) & (trip['tripduration'] <= 300*60):\n",
    "\t\t\taddl_duration = trip['tripduration'] - 270*60\n",
    "\t\t\tcost += 46.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 300*60) & (trip['tripduration'] <= 330*60):\n",
    "\t\t\taddl_duration = trip['tripduration'] - 300*60\n",
    "\t\t\tcost += 52.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 330*60) & (trip['tripduration'] <= 360*60):\n",
    "\t\t\taddl_duration = trip['tripduration'] - 330*60\n",
    "\t\t\tcost += 58.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 360*60) & (trip['tripduration'] <= 1440*60):\n",
    "\t\t\t# Each additional 6 hours: $76.50\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 360*60\n",
    "\t\t\tcost += 76.50*(addl_duration/(30.0*60))\n",
    "\t\tif (trip['tripduration'] > 1440*60):\n",
    "\t\t\t# Each additional 24 hours: $1200\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 1440*60\n",
    "\t\t\tcost += 1200*(addl_duration/(30.0*60))\n",
    "\t\treturn cost\n",
    "\telse:\n",
    "\t\tcost = 0\n",
    "\t\tif trip['tripduration'] > 30*60:\n",
    "\t\t\t# 30-60 minutes: $2.00\n",
    "\t\t\tcost += 2.0\n",
    "\t\tif trip['tripduration'] > 60*60:\n",
    "\t\t\t# 60-90 minutes: $4.00\n",
    "\t\t\tcost += 4.0\n",
    "\t\tif trip['tripduration'] > 90*60:\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $8.00\n",
    "\t\t\taddl_duration = trip['tripduration'] - 90*60\n",
    "\t\t\tcost += 8.0*(addl_duration/(30.0*60))\n",
    "\t\treturn cost\n",
    "\n",
    "\n",
    "def trip_cost_13_14(trip):\n",
    "\tif trip['usertype']=='Subscriber':\n",
    "\t\tcost = 0\n",
    "\t\tif trip['tripduration'] > 30*60:\n",
    "\t\t\t# 30-60 minutes: $1.50\n",
    "\t\t\tcost += 1.5\n",
    "\t\tif trip['tripduration'] > 60*60:\n",
    "\t\t\t# 60-90 minutes: +$3.00\n",
    "\t\t\tcost += 3.0\n",
    "\t\tif trip['tripduration'] > 90*60:\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $6.00\t\t\t\n",
    "\t\t\taddl_duration = trip['tripduration'] - 90*60\n",
    "\t\t\tcost += 6.0*(addl_duration/(30.0*60))\n",
    "\t\treturn cost\n",
    "\telse:\n",
    "\t\tcost = 0\n",
    "\t\tif trip['tripduration'] > 30*60:\n",
    "\t\t\t# 30-60 minutes: $2.00\n",
    "\t\t\tcost += 2.0\n",
    "\t\tif trip['tripduration'] > 60*60:\n",
    "\t\t\t# 60-90 minutes: $4.00\n",
    "\t\t\tcost += 4.0\n",
    "\t\tif trip['tripduration'] > 90*60:\n",
    "\t\t\t# Each additional 30 minutes past 90 minutes: $8.00\n",
    "\t\t\taddl_duration = trip['tripduration'] - 90*60\n",
    "\t\t\tcost += 8.0*(addl_duration/(30.0*60))\n",
    "\t\treturn cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#divvyTrips.loc[divvyTrips['Year'] == 2013, 'revenue'] = divvyTrips[divvyTrips.Year == 2013].apply(trip_cost_13_14,axis=1)\n",
    "#divvyTrips.loc[divvyTrips['Year'] == 2014, 'revenue'] = divvyTrips[divvyTrips.Year == 2014].apply(trip_cost_13_14,axis=1)\n",
    "#divvyTrips.loc[divvyTrips['Year'] == 2015, 'revenue'] = divvyTrips[divvyTrips.Year == 2015].apply(trip_cost_15_16_17,axis=1)\n",
    "#divvyTrips.loc[divvyTrips['Year'] == 2016, 'revenue'] = divvyTrips[divvyTrips.Year == 2016].apply(trip_cost_15_16_17,axis=1)\n",
    "#divvyTrips.loc[divvyTrips['Year'] == 2017, 'revenue'] = divvyTrips[divvyTrips.Year == 2017].apply(trip_cost_15_16_17,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Geomettry column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd # Reading csv file \n",
    "from shapely.geometry import Point # Shapely for converting latitude/longtitude to geometry\n",
    "import geopandas as gpd # To create GeodataFrame\n",
    "# creating a geometry column \n",
    "divvyTrips['geometry_start'] = [Point(xy) for xy in zip(divvyTrips['longitude_start'], divvyTrips['latitude_start'])]\n",
    "divvyTrips['geometry_end'] = [Point(xy) for xy in zip(divvyTrips['longitude_end'], divvyTrips['latitude_end'])]\n",
    "\n",
    "# Coordinate reference system : WGS84\n",
    "crs = {'init': 'epsg:4326'}\n",
    "# Creating a Geographic data frame \n",
    "gdf = gpd.GeoDataFrame(divvyTrips, crs=crs, geometry=divvyTrips.geometry_start)\n",
    "gdf.plot(marker='*', markersize=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>latitude_start</th>\n",
       "      <th>...</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>month_name</th>\n",
       "      <th>months-year</th>\n",
       "      <th>tripInMinutes</th>\n",
       "      <th>start_station_group</th>\n",
       "      <th>end_station_group</th>\n",
       "      <th>revenue</th>\n",
       "      <th>Hour</th>\n",
       "      <th>TypeOfDay</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4118</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-27 12:11:00</td>\n",
       "      <td>2013-06-27 12:16:00</td>\n",
       "      <td>316</td>\n",
       "      <td>85</td>\n",
       "      <td>Michigan Ave &amp; Oak St</td>\n",
       "      <td>41.900960</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>5.266667</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4095</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-27 12:06:00</td>\n",
       "      <td>2013-06-27 12:11:00</td>\n",
       "      <td>301</td>\n",
       "      <td>85</td>\n",
       "      <td>Michigan Ave &amp; Oak St</td>\n",
       "      <td>41.900960</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>5.016667</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4192</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-27 12:15:00</td>\n",
       "      <td>2013-06-27 12:16:00</td>\n",
       "      <td>60</td>\n",
       "      <td>28</td>\n",
       "      <td>Larrabee St &amp; Menomonee St</td>\n",
       "      <td>41.914680</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4275</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-27 14:44:00</td>\n",
       "      <td>2013-06-27 14:45:00</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>41.874640</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4291</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-27 14:58:00</td>\n",
       "      <td>2013-06-27 15:05:00</td>\n",
       "      <td>433</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>41.874640</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>7.216667</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4263</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-27 14:39:00</td>\n",
       "      <td>2013-06-27 14:40:00</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>Wood St &amp; Milwaukee Ave</td>\n",
       "      <td>41.907655</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>1.033333</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4288</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2013-06-27 14:56:00</td>\n",
       "      <td>2013-06-27 14:57:00</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>Clinton St &amp; Tilden St</td>\n",
       "      <td>41.875885</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>20 - 29</td>\n",
       "      <td>20 - 29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4289</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>2013-06-27 14:57:00</td>\n",
       "      <td>2013-06-27 15:05:00</td>\n",
       "      <td>487</td>\n",
       "      <td>32</td>\n",
       "      <td>Racine Ave &amp; Congress Pkwy</td>\n",
       "      <td>41.874640</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>8.116667</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>4316</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-27 15:06:00</td>\n",
       "      <td>2013-06-27 15:09:00</td>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>41.869417</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>4342</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-27 15:13:00</td>\n",
       "      <td>2013-06-27 15:27:00</td>\n",
       "      <td>852</td>\n",
       "      <td>19</td>\n",
       "      <td>Loomis St &amp; Taylor St</td>\n",
       "      <td>41.869417</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun</td>\n",
       "      <td>Jun-2013</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>10 - 19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  trip_id    usertype  gender           starttime  \\\n",
       "0           0     4118    Customer     NaN 2013-06-27 12:11:00   \n",
       "1           1     4095  Subscriber    Male 2013-06-27 12:06:00   \n",
       "2           2     4192  Subscriber    Male 2013-06-27 12:15:00   \n",
       "3           3     4275    Customer     NaN 2013-06-27 14:44:00   \n",
       "4           4     4291    Customer     NaN 2013-06-27 14:58:00   \n",
       "5           5     4263  Subscriber    Male 2013-06-27 14:39:00   \n",
       "6           6     4288  Subscriber    Male 2013-06-27 14:56:00   \n",
       "7           7     4289  Subscriber  Female 2013-06-27 14:57:00   \n",
       "8           8     4316    Customer     NaN 2013-06-27 15:06:00   \n",
       "9           9     4342    Customer     NaN 2013-06-27 15:13:00   \n",
       "\n",
       "              stoptime  tripduration  from_station_id  \\\n",
       "0  2013-06-27 12:16:00           316               85   \n",
       "1  2013-06-27 12:11:00           301               85   \n",
       "2  2013-06-27 12:16:00            60               28   \n",
       "3  2013-06-27 14:45:00            64               32   \n",
       "4  2013-06-27 15:05:00           433               32   \n",
       "5  2013-06-27 14:40:00            62               61   \n",
       "6  2013-06-27 14:57:00            66               68   \n",
       "7  2013-06-27 15:05:00           487               32   \n",
       "8  2013-06-27 15:09:00           123               19   \n",
       "9  2013-06-27 15:27:00           852               19   \n",
       "\n",
       "            from_station_name  latitude_start   ...    Quarter  month_name  \\\n",
       "0       Michigan Ave & Oak St       41.900960   ...          2         Jun   \n",
       "1       Michigan Ave & Oak St       41.900960   ...          2         Jun   \n",
       "2  Larrabee St & Menomonee St       41.914680   ...          2         Jun   \n",
       "3  Racine Ave & Congress Pkwy       41.874640   ...          2         Jun   \n",
       "4  Racine Ave & Congress Pkwy       41.874640   ...          2         Jun   \n",
       "5     Wood St & Milwaukee Ave       41.907655   ...          2         Jun   \n",
       "6      Clinton St & Tilden St       41.875885   ...          2         Jun   \n",
       "7  Racine Ave & Congress Pkwy       41.874640   ...          2         Jun   \n",
       "8       Loomis St & Taylor St       41.869417   ...          2         Jun   \n",
       "9       Loomis St & Taylor St       41.869417   ...          2         Jun   \n",
       "\n",
       "   months-year tripInMinutes  start_station_group  end_station_group  revenue  \\\n",
       "0     Jun-2013      5.266667              10 - 19            10 - 19      0.0   \n",
       "1     Jun-2013      5.016667              10 - 19            10 - 19      0.0   \n",
       "2     Jun-2013      1.000000              10 - 19            10 - 19      0.0   \n",
       "3     Jun-2013      1.066667              10 - 19            10 - 19      0.0   \n",
       "4     Jun-2013      7.216667              10 - 19            10 - 19      0.0   \n",
       "5     Jun-2013      1.033333              10 - 19            10 - 19      0.0   \n",
       "6     Jun-2013      1.100000              20 - 29            20 - 29      0.0   \n",
       "7     Jun-2013      8.116667              10 - 19            10 - 19      0.0   \n",
       "8     Jun-2013      2.050000              10 - 19            10 - 19      0.0   \n",
       "9     Jun-2013     14.200000              10 - 19            10 - 19      0.0   \n",
       "\n",
       "   Hour  TypeOfDay  season  \n",
       "0    12    Weekday  Summer  \n",
       "1    12    Weekday  Summer  \n",
       "2    12    Weekday  Summer  \n",
       "3    14    Weekday  Summer  \n",
       "4    14    Weekday  Summer  \n",
       "5    14    Weekday  Summer  \n",
       "6    14    Weekday  Summer  \n",
       "7    14    Weekday  Summer  \n",
       "8    15    Weekday  Summer  \n",
       "9    15    Weekday  Summer  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
